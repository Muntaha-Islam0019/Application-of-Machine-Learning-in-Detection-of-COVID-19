{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"D:/Study Materials/Practical Materials/Thesis Files/Thesis/data/train\"\n",
    "test_dir = \"D:/Study Materials/Practical Materials/Thesis Files/Thesis/data/train\"\n",
    "val_dir = \"D:/Study Materials/Practical Materials/Thesis Files/Thesis/data/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.python.keras.models import Model, Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For VGG16\n",
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.python.keras.applications.vgg16 import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For InceptionV3\n",
    "from tensorflow.python.keras.applications.inception_v3 import InceptionV3 \n",
    "from tensorflow.python.keras.applications.inception_v3  import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For ResNet50\n",
    "from tensorflow.python.keras.applications.resnet import ResNet50\n",
    "from tensorflow.python.keras.applications.resnet import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Explanable AI\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nOkK9XQafDFM"
   },
   "outputs": [],
   "source": [
    "def transferLayer(model, layer):\n",
    "    conv_model = Model(inputs=model.input, outputs=layer.output)\n",
    "    # Start a new Keras Sequential model\n",
    "    new_model = Sequential()\n",
    "\n",
    "    # Add the convolutional part of the VGG16 model from above\n",
    "    new_model.add(conv_model)\n",
    "\n",
    "    # Flatten the output of the VGG16 model because it is from a convolutional layer\n",
    "    new_model.add(Flatten())\n",
    "\n",
    "    # Add a dense (aka. fully-connected) layer\n",
    "    # This is for combining features that the VGG16 model has recognized in the image\n",
    "    new_model.add(Dropout(0.5))\n",
    "\n",
    "    new_model.add(Dense(512, activation=\"relu\"))\n",
    "\n",
    "    # Add the final layer for the actual classification\n",
    "    new_model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def trainableLayers(model):\n",
    "    for i in range(len(list(model.layers))):\n",
    "        print(list(model.layers)[i].name)\n",
    "    return list(model.layers)[-1].name\n",
    "\n",
    "\n",
    "def history(model, generator_train, generator_val):\n",
    "    optimizer = Adam(lr=1e-5)\n",
    "    loss = \"categorical_crossentropy\"\n",
    "    metrics = [\"categorical_accuracy\"]\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    epochs = 25\n",
    "    steps_per_epoch = (generator_train.n / batch_size) / 3\n",
    "    steps_val = generator_val.n / batch_size\n",
    "    history = model.fit_generator(\n",
    "        generator=generator_train,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=generator_val,\n",
    "        validation_steps=steps_val,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "\n",
    "def plotmodel(model):\n",
    "    return tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        dpi=96,\n",
    "    )\n",
    "\n",
    "\n",
    "def plotGraph(h):\n",
    "    pd.DataFrame(h.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1)\n",
    "    return plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "HqBOAUC5h-DQ",
    "outputId": "94f6d4ee-9623-4f95-f0cf-efb394f43ba1"
   },
   "outputs": [],
   "source": [
    "input_shape=224,224\n",
    "datagen_train = ImageDataGenerator(rescale=1.0 / 255)\n",
    "datagen_test = ImageDataGenerator(rescale=1.0 / 255)\n",
    "datagen_val = ImageDataGenerator(rescale=1.0 / 255)\n",
    "batch_size = 16\n",
    "generator_train = datagen_train.flow_from_directory(\n",
    "    directory=train_dir, target_size=input_shape, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "generator_test = datagen_test.flow_from_directory(\n",
    "    directory=test_dir, target_size=input_shape, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "generator_val = datagen_val.flow_from_directory(\n",
    "    directory=val_dir, target_size=input_shape, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanable AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xAI(model, img_dir):\n",
    "    image = Image.open(img_dir)\n",
    "    image = image.resize((224, 224))\n",
    "    img = np.asarray(image)\n",
    "    print(img.shape)\n",
    "    explainer = lime_image.LimeImageExplainer(random_state=42)\n",
    "    explanation = explainer.explain_instance(img, vgg16.predict)\n",
    "    image, mask = explanation.get_image_and_mask(\n",
    "        vgg16.predict(img.reshape((1, 224, 224, 3))).argmax(axis=1)[0],\n",
    "        positive_only=False,\n",
    "        hide_rest=False,\n",
    "    )\n",
    "    plt.imshow(mark_boundaries(image, mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = VGG16(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n",
    "l1 = trainableLayers(model1)\n",
    "print(\"Last Layer : \" + l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = model1.get_layer(l1)\n",
    "vgg16 = transferLayer(model1, layer1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-validation\n",
    "h1 = history(vgg16, generator_train, generator_val)\n",
    "plotGraph(h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Explanable AI on VGG16\n",
    "xAI(vgg16, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAI(vgg16, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAI(vgg16, \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xAI(vgg16, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = ResNet50(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n",
    "l4 = trainableLayers(model4)\n",
    "print(\"Last Layer : \" + l4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer4 = model4.get_layer(l4)\n",
    "r50 = transferLayer(model4, layer4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r50.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-validation\n",
    "h4 = history(r50, generator_train, generator_val)\n",
    "plotGraph(h4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = InceptionV3(include_top=False, input_shape=(224, 224, 3), weights=\"imagenet\")\n",
    "l3 = trainableLayers(model3)\n",
    "print(\"Last Layer : \" + l3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3 = model3.get_layer(l3)\n",
    "iv3 = transferLayer(model3, layer3)\n",
    "# plotmodel(iv3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv3.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-validation\n",
    "h3 = history(iv3, generator_train, generator_val)\n",
    "plotGraph(h3)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
